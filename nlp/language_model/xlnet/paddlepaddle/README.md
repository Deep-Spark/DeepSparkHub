# XLNet

## Model Description

XLNet is an advanced language model that combines the strengths of autoregressive and autoencoding approaches. It
introduces permutation language modeling, allowing the model to consider all possible word orders while maintaining the
autoregressive property. This enables XLNet to capture bidirectional context more effectively than traditional models.
Additionally, it incorporates Transformer-XL architecture, which handles long-range dependencies through segment
recurrence and relative positional encoding. XLNet achieves state-of-the-art performance across various NLP tasks by
leveraging these innovative techniques.

## Supported Environments

| GPU    | [IXUCA SDK](https://gitee.com/deep-spark/deepspark#%E5%A4%A9%E6%95%B0%E6%99%BA%E7%AE%97%E8%BD%AF%E4%BB%B6%E6%A0%88-ixuca) | Release |
|--------|-----------|---------|
| BI-V100 | 3.1.0     |  23.12  |

## Model Preparation

### Prepare Resources

The dataset included in the GLUE evaluation task has been provided in the form of API in PaddleNLP, no preparation is
required in advance. It will be automatically downloaded when executing using `run_glue.py`.

### Install Dependencies

```bash
pip3 install sentencepiece
pip3 install urllib3==1.26.6
pip3 install paddlenlp==2.4.1
```

## Model Training

Taking the SST-2 task in GLUE as an example, the method of starting Fine-tuning is as follows:

```bash
# Set --gpus to be "0" if run on 1 GPU
python3 -m paddle.distributed.launch --gpus "0,1,2,3,4,5,6,7" ./run_glue.py \
                                     --model_name_or_path xlnet-base-cased \
                                     --task_name SST-2 \
                                     --max_seq_length 128 \
                                     --batch_size 32 \
                                     --learning_rate 2e-5 \
                                     --num_train_epochs 3 \
                                     --logging_steps 100 \
                                     --save_steps 500 \
                                     --output_dir ./tmp/xlnet_model/
```

The parameters are explained as follows:

- `model_name_or_path` indicates a model with a specific configuration, corresponding to its pre-trained model and the
  tokenizer used in pre-training. If the model-related content is saved locally, the corresponding directory address can
  also be provided here.
- `task_name` indicates the task of Fine-tuning.
- `max_seq_length` indicates the maximum sentence length, beyond which it will be truncated.
- `batch_size` represents the number of samples per card per iteration.
- `learning_rate` indicates the size of the basic learning rate, which is multiplied with the value generated by the
  learning rate scheduler as the current learning rate.
- `num_train_epochs` indicates the number of training rounds.
- `logging_steps` indicates the log printing interval.
- `save_steps` indicates the model saving and evaluation interval.
- `output_dir` indicates the model saving path.

## Model Results

| Model | GPUs       | FPS   | ACC    |
|-------|------------|-------|--------|
| XLNet | BI-V100 x8 | 743.7 | 0.9450 |

## References

- [XLNet](https://arxiv.org/abs/1906.08237)
- [PaddleNLP](<https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/xlnet>

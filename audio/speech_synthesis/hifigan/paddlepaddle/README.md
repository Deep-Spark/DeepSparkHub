# PP-TTS-HiFiGAN

## Model Description

HiFiGAN is a commonly used vocoder in academia and industry in recent years, which can convert the frequency spectrum
generated by acoustic models into high-quality audio. This vocoder uses generative adversarial networks as the basis for
generating models.

## Supported Environments

| GPU    | [IXUCA SDK](https://gitee.com/deep-spark/deepspark#%E5%A4%A9%E6%95%B0%E6%99%BA%E7%AE%97%E8%BD%AF%E4%BB%B6%E6%A0%88-ixuca) | Release |
|--------|-----------|---------|
| BI-V100 | 3.1.0     |  23.09  |

## Model Preparation

### Prepare Resources

### Download and Extract

Download CSMSC(BZNSYP) from this [Website.](https://aistudio.baidu.com/datasetdetail/36741)and extract it to ./datasets.
Then the dataset is in the directory ./datasets/BZNSYP.

### Get MFA Result and Extract

We use [MFA](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner) to get durations for fastspeech2. You can
download from here
[baker_alignment_tone.tar.gz](https://paddlespeech.bj.bcebos.com/MFA/BZNSYP/with_tone/baker_alignment_tone.tar.gz).

Put the data directory structure like this:

```sh
voc5
├── baker_alignment_tone
├── conf
├── datasets
│   └── BZNSYP
│       ├── PhoneLabeling
│       ├── ProsodyLabeling
│       └── Wave
├── local
└── ...
```

Change the rootdir of dataset in ./local/preprocess.sh to the dataset path. Like this: `--rootdir=./datasets/BZNSYP`

### Install Dependencies

```sh
# Pip the requirements
pip3 install requirements.txt

# Clone the repo
git clone https://github.com/PaddlePaddle/PaddleSpeech.git
cd PaddleSpeech/examples/csmsc/voc5

```

### Preprocess Data

```sh
./run.sh --stage 0 --stop-stage 0
```

When it is done. A `dump` folder is created in the current directory. The structure of the dump folder is listed below.

```sh
dump
├── dev
│   ├── norm
│   └── raw
├── test
│   ├── norm
│   └── raw
└── train
    ├── norm
    ├── raw
    └── feats_stats.npy
```

### Model Training

You can choose use how many gpus for training by changing `gups` parameter in run.sh file and `ngpu` parameter in ./local/train.sh file.

Modify `./local/train.sh `file to use python3 run.

```sh
sed -i 's/python /python3 /g' ./local/train.sh
```

Full training may cost much time, you can modify the `train_max_steps` parameter in ./conf/default.yaml file to reduce
training time. But in order to get the weight file you should make the `train_max_steps` parameter bigger than
`save_interval_steps ` parameter.

```sh
./run.sh --stage 1 --stop-stage 1
```

### Synthesizing

Modify the parameter of `ckpt_name` in run.sh file to the weight name after training.

```sh
./run.sh --stage 2 --stop-stage 2
```

## Model Results

Main results after 1000 step train.

| GPUS        | avg_ips             | adversarial loss | feature matching loss | mel loss | generator loss | real loss | fake loss | discriminator loss |
|-------------|---------------------|------------------|-----------------------|----------|----------------|-----------|-----------|--------------------|
| BI V100 × 1 | 15.42 sequences/sec | 6.276            | 0.845                 | 0.531    | 31.858         | 0.513     | 0.6289    | 1.142              |

## References

- [HiFiGAN](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/csmsc/voc5)
